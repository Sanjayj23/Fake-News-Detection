{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhishek-1-Kumar/fake_news_detection/blob/main/Fake_News_Detection_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install Dependencies & Import Libraries"
      ],
      "metadata": {
        "id": "4Z6JDI3PIvhU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSXQAsTLw3VY"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets scikit-learn streamlit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBFqyZwhw7XD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Upload Data & Preprocess it"
      ],
      "metadata": {
        "id": "s2a4Hx5WI7zt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDsV5E1Yw9bG"
      },
      "outputs": [],
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fake_df_full = pd.read_csv('/content/drive/MyDrive/fake_news_detection/Fake.csv')\n",
        "true_df_full = pd.read_csv('/content/drive/MyDrive/fake_news_detection/True.csv')"
      ],
      "metadata": {
        "id": "7PNooh3c0FSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows where 'text' is NaN, empty string, or just whitespace\n",
        "true_df_clean = true_df_full.dropna(subset=[\"text\"])\n",
        "true_df_clean = true_df_clean[true_df_clean[\"text\"].str.strip() != \"\"]\n",
        "\n",
        "fake_df_clean = fake_df_full.dropna(subset=[\"text\"])\n",
        "fake_df_clean = fake_df_clean[fake_df_clean[\"text\"].str.strip() != \"\"]\n",
        "\n",
        "# Select top 100 clean rows from each\n",
        "true_df = true_df_clean.head(100).copy()\n",
        "fake_df = fake_df_clean.head(100).copy()\n",
        "\n",
        "# Add labels\n",
        "true_df[\"label\"] = 1\n",
        "fake_df[\"label\"] = 0\n",
        "\n",
        "# Combine and shuffle\n",
        "df = pd.concat([true_df, fake_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Keep only necessary columns\n",
        "df = df[[\"text\", \"label\"]]\n",
        "\n",
        "# Display confirmation\n",
        "print(f\"Loaded {len(df)} total samples (True: {len(true_df)}, Fake: {len(fake_df)})\")\n"
      ],
      "metadata": {
        "id": "SA58BmPh0cjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Tokenize and Prepare Dataset"
      ],
      "metadata": {
        "id": "zQ2iWFG8CKnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Step 1: First split into train (80%) and temp (20%)\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
        "\n",
        "# Step 2: Split temp into validation (10%) and test (10%)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n",
        "\n",
        "# Convert to Hugging Face datasets\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(\n",
        "        example[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "# Apply tokenization\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n"
      ],
      "metadata": {
        "id": "_tS6QcVw6v0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Safe Conversion (Avoid NumPy 2.0 Bug)"
      ],
      "metadata": {
        "id": "5rF7VSYCCQ25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to Python format\n",
        "train_list = train_dataset.with_format(\"python\")\n",
        "val_list = val_dataset.with_format(\"python\")\n",
        "test_list = test_dataset.with_format(\"python\")\n",
        "\n",
        "# Extract fields\n",
        "train_encodings = {\n",
        "    \"input_ids\": [ex[\"input_ids\"] for ex in train_list],\n",
        "    \"attention_mask\": [ex[\"attention_mask\"] for ex in train_list],\n",
        "    \"labels\": [ex[\"label\"] for ex in train_list]\n",
        "}\n",
        "val_encodings = {\n",
        "    \"input_ids\": [ex[\"input_ids\"] for ex in val_list],\n",
        "    \"attention_mask\": [ex[\"attention_mask\"] for ex in val_list],\n",
        "    \"labels\": [ex[\"label\"] for ex in val_list]\n",
        "}\n",
        "test_encodings = {\n",
        "    \"input_ids\": [ex[\"input_ids\"] for ex in test_list],\n",
        "    \"attention_mask\": [ex[\"attention_mask\"] for ex in test_list],\n",
        "    \"labels\": [ex[\"label\"] for ex in test_list]\n",
        "}\n"
      ],
      "metadata": {
        "id": "tFv_3XYbCPT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Define PyTorch Dataset"
      ],
      "metadata": {
        "id": "yPnHcKSWCZPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class FakeNewsDataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.input_ids = encodings[\"input_ids\"]\n",
        "        self.attention_mask = encodings[\"attention_mask\"]\n",
        "        self.labels = encodings[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(self.input_ids[idx]),\n",
        "            \"attention_mask\": torch.tensor(self.attention_mask[idx]),\n",
        "            \"labels\": torch.tensor(self.labels[idx])\n",
        "        }\n",
        "\n",
        "train_dataset = FakeNewsDataset(train_encodings)\n",
        "val_dataset = FakeNewsDataset(val_encodings)\n",
        "test_dataset = FakeNewsDataset(test_encodings)\n"
      ],
      "metadata": {
        "id": "lGi1Mk4LCXmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Load Model and Define Metrics"
      ],
      "metadata": {
        "id": "xueg9GljCllL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
      ],
      "metadata": {
        "id": "fUrP17vACffY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Training Arguments"
      ],
      "metadata": {
        "id": "LPF5M6gmCp2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    report_to=\"none\"  # disables wandb/tensorboard\n",
        ")\n"
      ],
      "metadata": {
        "id": "0gnfSg8kCojz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Train"
      ],
      "metadata": {
        "id": "AqA2dcMXC5uR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "KsHNZ9LoCx0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Evaluate on Test Set"
      ],
      "metadata": {
        "id": "kD75CjkGDE6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(test_dataset)\n"
      ],
      "metadata": {
        "id": "y-Ik9zGcC8rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Save Model"
      ],
      "metadata": {
        "id": "CO1ul6x2Jtm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a folder to store the model\n",
        "model_path = \"/content/drive/MyDrive/fake_news_detection/model\"\n",
        "\n",
        "# Save model and tokenizer\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n",
        "\n",
        "print(f\"Model saved to: {model_path}\")\n"
      ],
      "metadata": {
        "id": "6DFyMkZwDeJD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8M0NY3xdMhqDQqG25H71k",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}